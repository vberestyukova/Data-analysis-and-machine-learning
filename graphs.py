# -*- coding: utf-8 -*-
"""Graphs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ssJ-opELQodl9xoB_nB9L9oUFz0axJM
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import networkx as nx
from google.colab import drive
from scipy.stats import pearsonr as corr_coef
drive.mount('/content/drive')

"""Я прочитала файлы со своего Гугл диска. Создаем граф G и выпишем основные свойства: кол-во вершин, кол-во ребер, насыщенность"""

G=nx.read_edgelist('drive/MyDrive/bio-grid-worm.edges', delimiter=',') 
print('Nodes:',G.number_of_nodes()) 
print('Edges:',G.number_of_edges())
print('Density:',nx.density(G))

"""Тут выводится наименование вершины и их степень, т.е. сколько ребер с каждой из них связано"""

dict(G.degree())

"""А здесь можно вывести для конкретного значения"""

dict(G.degree())['521']

"""Выводим среднее, максимальное и минимальное значения степеней"""

degrees=[d for n, d in G.degree()]
print(np.mean(degrees))
print('Max degree:', np.max(degrees))
print('Min degree:', np.min(degrees))

a=plt.hist(degrees, bins=100, log=True)
plt.xlabel('k', fontsize=15)
plt.ylabel(r'$\rho(k)$', fontsize=15)

"""## Основные структурные свойства, устойчивость сети"""

lcc = nx.clustering(G)

b=plt.hist(lcc.values(),log=True, bins = 100)

"""Построим распределение коэффициента кластеризации"""

from collections import Counter
res = Counter(lcc.values())
res.most_common(8)

"""Видим, что больше всего 0 и 1. Также немало и других выведенных значений.

"""

res = Counter(lcc.values())
res.most_common(2)

"""Ответ: Из этого мы получили следующее: в основном принимаются значения 0 и 1, причем 2991 вершины имеют нулевую кластеризацию, а 1 - 69 вершины"""

gdict = dict(G.degree())
X=[]
Y=[]
for i in gdict.keys():
    Y.append(gdict[i])

for j in lcc.keys():
    X.append(lcc[j])  

pirson = np.corrcoef(X, Y)
pirson

"""Ответ: Мы получили коэффициент корреляции Пирсона, равный 0.029, что весьма мало, из этого следует, что корреляции нет (т. е. нет значимой зависимости)"""

print(nx.transitivity(G))
print(nx.is_connected(G))

"""Граф несвязный

LCG-подграф на списке вершин максимальной компоненты (она найдена выше LCG_nodes)
"""

LCG_nodes=max(nx.connected_components(G), key=len) #Блок с отступами nx.connected_components(G) возвращает генератор из объектов (из подграфов сети)
LCG=G.subgraph(LCG_nodes) 
print(len(LCG)/len(G))

"""95% - столько занимает максимальная компонента относительно исходного графа. """

len(LCG)

SCC = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]
len(SCC)

SCC.remove(len(LCG))

"""Удалили макс компоненту для наглядности и получили распределение оставшихся компонент связности"""

c=plt.hist(SCC,log=True)

"""Ответ: количество компонент связности равно 71. Размер наибольшей компоненты - 3343.

Будем удалять из графа случайно ребра, а потом смотреть изменения транзитивности (количества треугольников). Эпсилон-процент связи, которые мы убираем за 1 шаг. Предварительно мы скопировали G в H. Выполним 10 итераций
"""

glob=nx.global_efficiency(LCG)
loc=nx.local_efficiency(LCG)

print(glob)
print(loc)

epsilon=0.05 # доля удаляемых ребер за один шаг
n=int(epsilon*G.number_of_edges()) 
H=G.copy()
tr=[]
tr.append(nx.transitivity(H)) #записываем треугольники
for i in range(10):
  edges_removed=[list(H.edges)[k] for k in np.random.choice(H.number_of_edges(),n, replace=False)]
  H.remove_edges_from(edges_removed)
  tr.append(nx.transitivity(H))

plt.plot(np.linspace(0,10*epsilon,11),tr,'o--')
plt.xlabel('fraction of removed links')#доля связи, которую мы убрали
plt.ylabel('transitivity')

"""Ответ: чем сильнее мы отбрасываем связи, тем сильнее падает транзитивность. На всем промежутку получаем монотонное убывание.

# Распределение степеней и корреляция

Построим распределение с помощью np.histogram
"""

degrees=[d for n, d in G.degree()] # лист всех значений
h,x=np.histogram(degrees,bins=100,density=True) #h-высота столбика, x-координата начала

x_=(x[:-1]+x[1:])/2 # для удобства перейдем к координатам середин столбиков
plt.plot(x_,h,'o--')
plt.xscale('log')#изменяем масштаб осей
plt.yscale('log')
plt.xlabel('k', fontsize=14)
z = np.polyfit(np.log(x_[np.nonzero(h)]), np.log(h[np.nonzero(h)]), 1) #строим линию тренда с помощью данной функции, указываем, что мы игнорируем столбики с 0 высотой
f= np.poly1d(z) # функция с найденными коэффициентами
plt.plot(x_, np.exp(f(np.log(x_))), color='red')#рисуем линию тренда
plt.ylabel(r'$\rho(k)$', fontsize=14)

dx=x_[1]-x_[0] #используем для нормировки
ccdf=1-(np.cumsum(h)*dx)#эта функция выдает на каждом этапе частные суммы
plt.plot(x_[:-1],ccdf[:-1],'o--')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('k', fontsize=14)
plt.ylabel(r'$\rho(k)$', fontsize=14)

z = np.polyfit(np.log(x_[:-10]), np.log(ccdf[:-10]), 1) #отсюда выкидываем последние точки
f= np.poly1d(z)
plt.plot(x_, np.exp(f(np.log(x_))), color='red')
print(z)

"""Такая функция менее чувствительна к шуму, сглаживаем эти выбросы (в 1 графике они весьма сильно влияют на поведение тренда, люой скачек изменял угол направления прямой). Сравнивая графики, видим, что сильно поменялся наклон"""

!pip install powerlaw #поможет определить, как спадают хвосты распределения
import powerlaw

fig, ax = plt.subplots(figsize = (14,10))

fit = powerlaw.Fit(degrees, discrete=True)#передаем набор степеней узлов

fit.distribution_compare('lognormal', 'truncated_power_law')#тут выбирается лучшее распределение
fit.plot_ccdf(ax=ax, linewidth=3, label='Empirical Data')
fit.lognormal.plot_ccdf(ax=ax, color='r', linestyle='--', label='Lognormal fit')
fit.truncated_power_law.plot_ccdf(ax=ax, color='g', linestyle='--', label='Truncated power law fit')
fit.distribution_compare('power_law', 'lognormal')
fit.power_law.plot_ccdf(ax=ax, color='k', linestyle='--', label='Power law fit')
#на графике нарисованы построены различные распределения
ax.set_ylabel(u"P(K≥k)",fontsize=16)
ax.set_xlabel("k",fontsize=16)
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles, labels, loc=3,fontsize=14)

"""Линейный коэффициент корреляции Пирсона двух массивов называется ассортативностью сети"""

print(nx.degree_assortativity_coefficient(G)) #ассортативность-высокосвязные узлы связываются также с высокосвязными. у нас отрицат.зн. =>диссартотивность, связь высокосвязных с низкосвязными

"""*Постройте функцию корреляции степеней. Для этого для каждого значения степени  k  определите среднее значение средней степени ближайших соседей вершин*"""

left_degree=[G.degree[P] for (P,Q) in G.edges()] #берем степень левого узла
right_degree=[G.degree[Q] for (P,Q) in G.edges()] #берем степень правого узла

l=left_degree+right_degree # в ненаправленной сети все ребра нужно пройти дважды в "обоих направлениях"
r=right_degree+left_degree
print(corr_coef(l,r)[0])

nn_degrees=nx.average_neighbor_degree(G).values()#среднее значение ближайшик соседей
plt.hist(nx.average_neighbor_degree(G).values(), bins=20)

nn_degrees #ср.зн. соседа

"""Это что-то вроде клуба избранных. Узлы с большими степенями связываются друг с другом очень плотно. Переход из одного пукта в другой будет происзводится через эти узлы. ВЫделяется такая подсеть. Тут используются ребра со степенями вершин >=k. Т.е. это плотность сети после убирания ущлов с степенями вершин меньших к

"""

RC=nx.rich_club_coefficient(G, normalized=False, seed=42)
k=list(RC.keys())
rho=list(RC.values())
plt.plot(k, rho, 'g')
plt.xlabel('k', fontsize=14)
plt.ylabel('RichClub(k)', fontsize=14)

def core(g):
    l_core = g.number_of_nodes()
    k = 0
    ratio = []
    while l_core:
        l_core = len(nx.k_core(g,k=k).nodes())
        ratio.append(l_core / g.number_of_nodes())
        k += 1
    return ratio      
ratio = core(G)
P=[]
i=0
for i in range(len(ratio)):
  P.append(i)
  i+=1

plt.plot(P, ratio, 'o--')
plt.ylabel('соотн. степеней', fontsize=15)
plt.xlabel('k', fontsize=15)
plt.grid(which='both')

"""# Моделирование и рандомизация"""

print('Number of nodes =',G.number_of_nodes(),'\n') #моя сеть
print('Number of edges =',G.number_of_edges(),'\n')

degrees=[d for node, d in G.degree()]
print ('Average degree =',np.mean(degrees),'\n')
print('Average clusteing:', nx.average_clustering(G),'\n')
print('Density:',nx.density(G), '\n')
print('Transitivity:',nx.transitivity(G))

gnm = nx.gnm_random_graph(3507, 6531)
degrees=[d for node, d in gnm.degree()]
print ('Average degree =',np.mean(degrees), '\n')
print('Average cluseting:', nx.average_clustering(gnm), '\n')
print('Density:',nx.density(gnm), '\n')
print('Transitivity:',nx.transitivity(gnm))

bag=nx.barabasi_albert_graph(3507,10)
degrees=[d for node, d in bag.degree()]
print ('Average degree =',np.mean(degrees), '\n')
print('Average cluseting:', nx.average_clustering(bag), '\n')
print('Density:',nx.density(bag),'\n')
print('Transitivity:',nx.transitivity(bag))

"""Сравниваем результаты с исходными. В модели Эрдеша-Реньи средняя степень и плотность совпадают с данными графа, очень близко значение транзитивности, у модели Барабаши-Альберта сильно отличается. Но модель Барабаши-Альберта лучше описала ср. кластеризацию. Ответ: ЭР лучше описывает среднюю степень, транзитивность и плотность. БА лучше описывает ср. кластеризацию"""

CM=nx.configuration_model(list(dict(G.degree()).values()),seed=42)
degrees=[d for node, d in CM.degree()]
CmGR=nx.Graph(CM)
print ('Average degree =',np.mean(degrees), '\n')
print('Average cluseting:', nx.average_clustering(CmGR), '\n')
print('Density:',nx.density(CM), '\n')
print('Transitivity:',nx.transitivity(CmGR))

"""Отлично описывает все свойства"""

pair = set(CM.edges())
multi = 0
loop = 0
print(pair)
for i in pair:
    if CM.number_of_edges(*i) > 1:
        multi += 1
    if i[0]==i[1]:
      loop+=1    
print(multi)
print(loop)

"""Ответ: кол-во петель 10, кол-во мультиребер 197"""

def random(g, f):
    z = g.copy()
    edges = list(z.edges())
    for k in range(f*z.number_of_edges() + 1):
        i, j = np.random.choice(len(edges), 2, replace=False)
        A, B = edges[i]
        C, D = edges[j]
        while z.has_edge(A, D) or z.has_edge(B, C) or A == D or B == C:
            i, j = np.random.choice(len(edges), 2, replace=False)
            A, B = edges[i]
            C, D = edges[j]
        z.remove_edge(*edges[i])
        z.remove_edge(*edges[j])
        z.add_edge(A, D)
        z.add_edge(B, C)
        edges.remove((A, B))
        edges.remove((C, D))
        edges.extend([(A, D), (B, C)])

    return z

f = 7
tran=[]
dens=[]
clust=[]

for i in range(f):
  z = random(G, f)
  tran.append(nx.transitivity(z))
  dens.append(nx.density(z))
  clust.append(nx.average_clustering(z))

print(tran)
print(dens)
print(clust)

"""Делаем Q шагов, в которых берем две рандомные пары вершин, проверяем их на связь(были, допустим, связаны AB и СD, хотим мы переставить связи, чтобы было AD, BC, нужно проверить, нет ли у нас уже связи AD или BC). Затем меняем узлы, добавляем новые, удаляем старые"""

plt.plot(list(range(f)), clust, 'o--')
plt.ylabel('Average clustering', fontsize=15)
plt.xlabel('f', fontsize=15)

plt.plot(list(range(f)), dens, 'o--')
plt.ylabel('density', fontsize=15)
plt.xlabel('f', fontsize=15)
plt.grid(which='both')

plt.plot(list(range(f)), tran, 'o--')
plt.ylabel('transitivity', fontsize=15)
plt.xlabel('f', fontsize=15)